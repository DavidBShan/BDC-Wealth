{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from keras>=3.5.0->tensorflow) (13.7.0)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/davidshan/Library/Python/3.12/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=2):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_country_data(country_data, sequence_length=5):\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(country_data.reshape(-1, 1))\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - sequence_length):\n",
    "        X.append(scaled_data[i:(i + sequence_length)])\n",
    "        y.append(scaled_data[i + sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y), scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, epochs=200):\n",
    "    # Create dataset and dataloader\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_tensor = torch.FloatTensor(y)\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "    train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Predictor(input_size=1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, last_sequence, scaler, n_future=5):\n",
    "    model.eval()\n",
    "    current_sequence = last_sequence.copy()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_future):\n",
    "            sequence = torch.FloatTensor(current_sequence).unsqueeze(0)\n",
    "            pred = model(sequence)\n",
    "            predictions.append(pred.numpy())\n",
    "            current_sequence = np.vstack((current_sequence[1:], pred.numpy()))\n",
    "    \n",
    "    predictions = np.array(predictions).reshape(-1, 1)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    \n",
    "    return predictions.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>code</th>\n",
       "      <th>Internet Usage (% of Population) 1990</th>\n",
       "      <th>Internet Usage (% of Population) 1991</th>\n",
       "      <th>Internet Usage (% of Population) 1992</th>\n",
       "      <th>Internet Usage (% of Population) 1993</th>\n",
       "      <th>Internet Usage (% of Population) 1994</th>\n",
       "      <th>Internet Usage (% of Population) 1995</th>\n",
       "      <th>Internet Usage (% of Population) 1996</th>\n",
       "      <th>Internet Usage (% of Population) 1997</th>\n",
       "      <th>...</th>\n",
       "      <th>Internet Usage (% of Population) 2014</th>\n",
       "      <th>Internet Usage (% of Population) 2015</th>\n",
       "      <th>Internet Usage (% of Population) 2016</th>\n",
       "      <th>Internet Usage (% of Population) 2017</th>\n",
       "      <th>Internet Usage (% of Population) 2018</th>\n",
       "      <th>Internet Usage (% of Population) 2019</th>\n",
       "      <th>Internet Usage (% of Population) 2020</th>\n",
       "      <th>Internet Usage (% of Population) 2021</th>\n",
       "      <th>Internet Usage (% of Population) 2022</th>\n",
       "      <th>Internet Usage (% of Population) 2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032196828</td>\n",
       "      <td>0.048593919</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>8.26</td>\n",
       "      <td>11</td>\n",
       "      <td>13.5</td>\n",
       "      <td>16.8</td>\n",
       "      <td>17.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>79.3237</td>\n",
       "      <td>82.6137</td>\n",
       "      <td>83.1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011168695</td>\n",
       "      <td>0.032196828</td>\n",
       "      <td>0.048593919</td>\n",
       "      <td>...</td>\n",
       "      <td>54.3</td>\n",
       "      <td>56.9</td>\n",
       "      <td>59.6</td>\n",
       "      <td>62.4</td>\n",
       "      <td>65.4</td>\n",
       "      <td>68.5504</td>\n",
       "      <td>72.2377</td>\n",
       "      <td>79.3237</td>\n",
       "      <td>82.6137</td>\n",
       "      <td>83.1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000360674</td>\n",
       "      <td>0.001768954</td>\n",
       "      <td>0.001738533</td>\n",
       "      <td>0.010268463</td>\n",
       "      <td>...</td>\n",
       "      <td>29.5</td>\n",
       "      <td>38.2</td>\n",
       "      <td>42.9455</td>\n",
       "      <td>47.6911</td>\n",
       "      <td>49.0385</td>\n",
       "      <td>58.9776</td>\n",
       "      <td>60.6534</td>\n",
       "      <td>66.2356</td>\n",
       "      <td>71.2432</td>\n",
       "      <td>83.1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>ASM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001738533</td>\n",
       "      <td>0.010268463</td>\n",
       "      <td>...</td>\n",
       "      <td>29.5</td>\n",
       "      <td>38.2</td>\n",
       "      <td>42.9455</td>\n",
       "      <td>47.6911</td>\n",
       "      <td>49.0385</td>\n",
       "      <td>58.9776</td>\n",
       "      <td>60.6534</td>\n",
       "      <td>66.2356</td>\n",
       "      <td>71.2432</td>\n",
       "      <td>83.1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>AND</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.526601023</td>\n",
       "      <td>3.050175385</td>\n",
       "      <td>...</td>\n",
       "      <td>86.1</td>\n",
       "      <td>87.9</td>\n",
       "      <td>89.7</td>\n",
       "      <td>91.5675</td>\n",
       "      <td>49.0385</td>\n",
       "      <td>90.7187</td>\n",
       "      <td>93.2056</td>\n",
       "      <td>93.8975</td>\n",
       "      <td>94.4855</td>\n",
       "      <td>83.1356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country code Internet Usage (% of Population) 1990  \\\n",
       "0     Afghanistan  AFG                                     0   \n",
       "1         Albania  ALB                                     0   \n",
       "2         Algeria  DZA                                     0   \n",
       "3  American Samoa  ASM                                     0   \n",
       "4         Andorra  AND                                     0   \n",
       "\n",
       "  Internet Usage (% of Population) 1991 Internet Usage (% of Population) 1992  \\\n",
       "0                                     0                                     0   \n",
       "1                                     0                                     0   \n",
       "2                                     0                                     0   \n",
       "3                                     0                                     0   \n",
       "4                                     0                                     0   \n",
       "\n",
       "  Internet Usage (% of Population) 1993 Internet Usage (% of Population) 1994  \\\n",
       "0                                     0                                     0   \n",
       "1                                     0                                     0   \n",
       "2                                     0                           0.000360674   \n",
       "3                                     0                                     0   \n",
       "4                                     0                                     0   \n",
       "\n",
       "  Internet Usage (% of Population) 1995 Internet Usage (% of Population) 1996  \\\n",
       "0                                     0                           0.032196828   \n",
       "1                           0.011168695                           0.032196828   \n",
       "2                           0.001768954                           0.001738533   \n",
       "3                                     0                           0.001738533   \n",
       "4                                     0                           1.526601023   \n",
       "\n",
       "  Internet Usage (% of Population) 1997  ...  \\\n",
       "0                           0.048593919  ...   \n",
       "1                           0.048593919  ...   \n",
       "2                           0.010268463  ...   \n",
       "3                           0.010268463  ...   \n",
       "4                           3.050175385  ...   \n",
       "\n",
       "  Internet Usage (% of Population) 2014 Internet Usage (% of Population) 2015  \\\n",
       "0                                     7                                  8.26   \n",
       "1                                  54.3                                  56.9   \n",
       "2                                  29.5                                  38.2   \n",
       "3                                  29.5                                  38.2   \n",
       "4                                  86.1                                  87.9   \n",
       "\n",
       "  Internet Usage (% of Population) 2016 Internet Usage (% of Population) 2017  \\\n",
       "0                                    11                                  13.5   \n",
       "1                                  59.6                                  62.4   \n",
       "2                               42.9455                               47.6911   \n",
       "3                               42.9455                               47.6911   \n",
       "4                                  89.7                               91.5675   \n",
       "\n",
       "  Internet Usage (% of Population) 2018 Internet Usage (% of Population) 2019  \\\n",
       "0                                  16.8                                  17.6   \n",
       "1                                  65.4                               68.5504   \n",
       "2                               49.0385                               58.9776   \n",
       "3                               49.0385                               58.9776   \n",
       "4                               49.0385                               90.7187   \n",
       "\n",
       "  Internet Usage (% of Population) 2020 Internet Usage (% of Population) 2021  \\\n",
       "0                                  18.4                               79.3237   \n",
       "1                               72.2377                               79.3237   \n",
       "2                               60.6534                               66.2356   \n",
       "3                               60.6534                               66.2356   \n",
       "4                               93.2056                               93.8975   \n",
       "\n",
       "  Internet Usage (% of Population) 2022 Internet Usage (% of Population) 2023  \n",
       "0                               82.6137                               83.1356  \n",
       "1                               82.6137                               83.1356  \n",
       "2                               71.2432                               83.1356  \n",
       "3                               71.2432                               83.1356  \n",
       "4                               94.4855                               83.1356  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/internet.csv')\n",
    "df.head()\n",
    "df.replace(\"..\", pd.NA, inplace=True)\n",
    "# Forward fill first, then backward fill to handle any remaining NAs at the start\n",
    "df.rename(columns={'entity': 'Country'}, inplace=True)\n",
    "df = df.ffill().bfill()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in df.columns if 'Internet' in col]\n",
    "    \n",
    "sequence_length = 5\n",
    "predictions_by_country = {}\n",
    "selected_countries = ['United States', 'China', 'Japan', 'Germany', 'United Kingdom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for Afghanistan\n",
      "Epoch [50/200], Loss: 0.0145\n",
      "Epoch [100/200], Loss: 0.0098\n",
      "Epoch [150/200], Loss: 0.0093\n",
      "Epoch [200/200], Loss: 0.0076\n",
      "\n",
      "Training model for Albania\n",
      "Epoch [50/200], Loss: 0.0106\n",
      "Epoch [100/200], Loss: 0.0078\n",
      "Epoch [150/200], Loss: 0.0037\n",
      "Epoch [200/200], Loss: 0.0019\n",
      "\n",
      "Training model for Algeria\n",
      "Epoch [50/200], Loss: 0.0041\n",
      "Epoch [100/200], Loss: 0.0015\n",
      "Epoch [150/200], Loss: 0.0014\n",
      "Epoch [200/200], Loss: 0.0014\n",
      "\n",
      "Training model for American Samoa\n",
      "Epoch [50/200], Loss: 0.0026\n",
      "Epoch [100/200], Loss: 0.0012\n",
      "Epoch [150/200], Loss: 0.0012\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "\n",
      "Training model for Andorra\n",
      "Epoch [50/200], Loss: 0.0258\n",
      "Epoch [100/200], Loss: 0.0119\n",
      "Epoch [150/200], Loss: 0.0127\n",
      "Epoch [200/200], Loss: 0.0090\n",
      "\n",
      "Training model for Angola\n",
      "Epoch [50/200], Loss: 0.0069\n",
      "Epoch [100/200], Loss: 0.0083\n",
      "Epoch [150/200], Loss: 0.0061\n",
      "Epoch [200/200], Loss: 0.0073\n",
      "\n",
      "Training model for Antigua and Barbuda\n",
      "Epoch [50/200], Loss: 0.0021\n",
      "Epoch [100/200], Loss: 0.0011\n",
      "Epoch [150/200], Loss: 0.0009\n",
      "Epoch [200/200], Loss: 0.0008\n",
      "\n",
      "Training model for Argentina\n",
      "Epoch [50/200], Loss: 0.0040\n",
      "Epoch [100/200], Loss: 0.0010\n",
      "Epoch [150/200], Loss: 0.0012\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "\n",
      "Training model for Armenia\n",
      "Epoch [50/200], Loss: 0.0053\n",
      "Epoch [100/200], Loss: 0.0022\n",
      "Epoch [150/200], Loss: 0.0021\n",
      "Epoch [200/200], Loss: 0.0022\n",
      "\n",
      "Training model for Aruba\n",
      "Epoch [50/200], Loss: 0.0176\n",
      "Epoch [100/200], Loss: 0.0080\n",
      "Epoch [150/200], Loss: 0.0073\n",
      "Epoch [200/200], Loss: 0.0068\n",
      "\n",
      "Training model for Australia\n",
      "Epoch [50/200], Loss: 0.0238\n",
      "Epoch [100/200], Loss: 0.0204\n",
      "Epoch [150/200], Loss: 0.0187\n",
      "Epoch [200/200], Loss: 0.0165\n",
      "\n",
      "Training model for Austria\n",
      "Epoch [50/200], Loss: 0.0051\n",
      "Epoch [100/200], Loss: 0.0026\n",
      "Epoch [150/200], Loss: 0.0018\n",
      "Epoch [200/200], Loss: 0.0017\n",
      "\n",
      "Training model for Azerbaijan\n",
      "Epoch [50/200], Loss: 0.0342\n",
      "Epoch [100/200], Loss: 0.0297\n",
      "Epoch [150/200], Loss: 0.0250\n",
      "Epoch [200/200], Loss: 0.0187\n",
      "\n",
      "Training model for Bahamas, The\n",
      "Epoch [50/200], Loss: 0.0089\n",
      "Epoch [100/200], Loss: 0.0042\n",
      "Epoch [150/200], Loss: 0.0039\n",
      "Epoch [200/200], Loss: 0.0041\n",
      "\n",
      "Training model for Bahrain\n",
      "Epoch [50/200], Loss: 0.0029\n",
      "Epoch [100/200], Loss: 0.0026\n",
      "Epoch [150/200], Loss: 0.0024\n",
      "Epoch [200/200], Loss: 0.0025\n",
      "\n",
      "Training model for Bangladesh\n",
      "Epoch [50/200], Loss: 0.0033\n",
      "Epoch [100/200], Loss: 0.0006\n",
      "Epoch [150/200], Loss: 0.0007\n",
      "Epoch [200/200], Loss: 0.0004\n",
      "\n",
      "Training model for Barbados\n",
      "Epoch [50/200], Loss: 0.0236\n",
      "Epoch [100/200], Loss: 0.0135\n",
      "Epoch [150/200], Loss: 0.0101\n",
      "Epoch [200/200], Loss: 0.0057\n",
      "\n",
      "Training model for Belarus\n",
      "Epoch [50/200], Loss: 0.0284\n",
      "Epoch [100/200], Loss: 0.0213\n",
      "Epoch [150/200], Loss: 0.0167\n",
      "Epoch [200/200], Loss: 0.0126\n",
      "\n",
      "Training model for Belgium\n",
      "Epoch [50/200], Loss: 0.0081\n",
      "Epoch [100/200], Loss: 0.0040\n",
      "Epoch [150/200], Loss: 0.0023\n",
      "Epoch [200/200], Loss: 0.0009\n",
      "\n",
      "Training model for Belize\n",
      "Epoch [50/200], Loss: 0.0097\n",
      "Epoch [100/200], Loss: 0.0088\n",
      "Epoch [150/200], Loss: 0.0113\n",
      "Epoch [200/200], Loss: 0.0084\n",
      "\n",
      "Training model for Benin\n",
      "Epoch [50/200], Loss: 0.0083\n",
      "Epoch [100/200], Loss: 0.0122\n",
      "Epoch [150/200], Loss: 0.0066\n",
      "Epoch [200/200], Loss: 0.0091\n",
      "\n",
      "Training model for Bermuda\n",
      "Epoch [50/200], Loss: 0.0621\n",
      "Epoch [100/200], Loss: 0.0430\n",
      "Epoch [150/200], Loss: 0.0395\n",
      "Epoch [200/200], Loss: 0.0295\n",
      "\n",
      "Training model for Bhutan\n",
      "Epoch [50/200], Loss: 0.0178\n",
      "Epoch [100/200], Loss: 0.0118\n",
      "Epoch [150/200], Loss: 0.0115\n",
      "Epoch [200/200], Loss: 0.0072\n",
      "\n",
      "Training model for Bolivia\n",
      "Epoch [50/200], Loss: 0.0040\n",
      "Epoch [100/200], Loss: 0.0040\n",
      "Epoch [150/200], Loss: 0.0034\n",
      "Epoch [200/200], Loss: 0.0028\n",
      "\n",
      "Training model for Bosnia and Herzegovina\n",
      "Epoch [50/200], Loss: 0.0059\n",
      "Epoch [100/200], Loss: 0.0044\n",
      "Epoch [150/200], Loss: 0.0043\n",
      "Epoch [200/200], Loss: 0.0035\n",
      "\n",
      "Training model for Botswana\n",
      "Epoch [50/200], Loss: 0.0055\n",
      "Epoch [100/200], Loss: 0.0036\n",
      "Epoch [150/200], Loss: 0.0036\n",
      "Epoch [200/200], Loss: 0.0030\n",
      "\n",
      "Training model for Brazil\n",
      "Epoch [50/200], Loss: 0.0046\n",
      "Epoch [100/200], Loss: 0.0027\n",
      "Epoch [150/200], Loss: 0.0024\n",
      "Epoch [200/200], Loss: 0.0021\n",
      "\n",
      "Training model for British Virgin Islands\n",
      "Epoch [50/200], Loss: 0.0071\n",
      "Epoch [100/200], Loss: 0.0054\n",
      "Epoch [150/200], Loss: 0.0047\n",
      "Epoch [200/200], Loss: 0.0052\n",
      "\n",
      "Training model for Brunei Darussalam\n",
      "Epoch [50/200], Loss: 0.0075\n",
      "Epoch [100/200], Loss: 0.0050\n",
      "Epoch [150/200], Loss: 0.0048\n",
      "Epoch [200/200], Loss: 0.0043\n",
      "\n",
      "Training model for Bulgaria\n",
      "Epoch [50/200], Loss: 0.0054\n",
      "Epoch [100/200], Loss: 0.0028\n",
      "Epoch [150/200], Loss: 0.0021\n",
      "Epoch [200/200], Loss: 0.0014\n",
      "\n",
      "Training model for Burkina Faso\n",
      "Epoch [50/200], Loss: 0.0102\n",
      "Epoch [100/200], Loss: 0.0067\n",
      "Epoch [150/200], Loss: 0.0045\n",
      "Epoch [200/200], Loss: 0.0024\n",
      "\n",
      "Training model for Burundi\n",
      "Epoch [50/200], Loss: 0.0283\n",
      "Epoch [100/200], Loss: 0.0067\n",
      "Epoch [150/200], Loss: 0.0033\n",
      "Epoch [200/200], Loss: 0.0010\n",
      "\n",
      "Training model for Cabo Verde\n",
      "Epoch [50/200], Loss: 0.0034\n",
      "Epoch [100/200], Loss: 0.0026\n",
      "Epoch [150/200], Loss: 0.0022\n",
      "Epoch [200/200], Loss: 0.0020\n",
      "\n",
      "Training model for Cambodia\n",
      "Epoch [50/200], Loss: 0.0169\n",
      "Epoch [100/200], Loss: 0.0057\n",
      "Epoch [150/200], Loss: 0.0042\n",
      "Epoch [200/200], Loss: 0.0036\n",
      "\n",
      "Training model for Cameroon\n",
      "Epoch [50/200], Loss: 0.0027\n",
      "Epoch [100/200], Loss: 0.0026\n",
      "Epoch [150/200], Loss: 0.0040\n",
      "Epoch [200/200], Loss: 0.0029\n",
      "\n",
      "Training model for Canada\n",
      "Epoch [50/200], Loss: 0.0077\n",
      "Epoch [100/200], Loss: 0.0058\n",
      "Epoch [150/200], Loss: 0.0047\n",
      "Epoch [200/200], Loss: 0.0031\n",
      "\n",
      "Training model for Cayman Islands\n",
      "Epoch [50/200], Loss: 0.0148\n",
      "Epoch [100/200], Loss: 0.0141\n",
      "Epoch [150/200], Loss: 0.0136\n",
      "Epoch [200/200], Loss: 0.0123\n",
      "\n",
      "Training model for Central African Republic\n",
      "Epoch [50/200], Loss: 0.0176\n",
      "Epoch [100/200], Loss: 0.0166\n",
      "Epoch [150/200], Loss: 0.0151\n",
      "Epoch [200/200], Loss: 0.0155\n",
      "\n",
      "Training model for Chad\n",
      "Epoch [50/200], Loss: 0.0264\n",
      "Epoch [100/200], Loss: 0.0151\n",
      "Epoch [150/200], Loss: 0.0153\n",
      "Epoch [200/200], Loss: 0.0119\n",
      "\n",
      "Training model for Channel Islands\n",
      "Epoch [50/200], Loss: 0.0169\n",
      "Epoch [100/200], Loss: 0.0143\n",
      "Epoch [150/200], Loss: 0.0151\n",
      "Epoch [200/200], Loss: 0.0122\n",
      "\n",
      "Training model for Chile\n",
      "Epoch [50/200], Loss: 0.0035\n",
      "Epoch [100/200], Loss: 0.0030\n",
      "Epoch [150/200], Loss: 0.0033\n",
      "Epoch [200/200], Loss: 0.0031\n",
      "\n",
      "Training model for China\n",
      "Epoch [50/200], Loss: 0.0041\n",
      "Epoch [100/200], Loss: 0.0024\n",
      "Epoch [150/200], Loss: 0.0021\n",
      "Epoch [200/200], Loss: 0.0015\n",
      "\n",
      "Training model for Colombia\n",
      "Epoch [50/200], Loss: 0.0057\n",
      "Epoch [100/200], Loss: 0.0012\n",
      "Epoch [150/200], Loss: 0.0007\n",
      "Epoch [200/200], Loss: 0.0006\n",
      "\n",
      "Training model for Comoros\n",
      "Epoch [50/200], Loss: 0.0175\n",
      "Epoch [100/200], Loss: 0.0174\n",
      "Epoch [150/200], Loss: 0.0160\n",
      "Epoch [200/200], Loss: 0.0151\n",
      "\n",
      "Training model for Congo, Dem. Rep.\n",
      "Epoch [50/200], Loss: 0.0068\n",
      "Epoch [100/200], Loss: 0.0035\n",
      "Epoch [150/200], Loss: 0.0040\n",
      "Epoch [200/200], Loss: 0.0026\n",
      "\n",
      "Training model for Congo, Rep.\n",
      "Epoch [50/200], Loss: 0.0027\n",
      "Epoch [100/200], Loss: 0.0012\n",
      "Epoch [150/200], Loss: 0.0012\n",
      "Epoch [200/200], Loss: 0.0009\n",
      "\n",
      "Training model for Costa Rica\n",
      "Epoch [50/200], Loss: 0.0036\n",
      "Epoch [100/200], Loss: 0.0019\n",
      "Epoch [150/200], Loss: 0.0022\n",
      "Epoch [200/200], Loss: 0.0018\n",
      "\n",
      "Training model for Cote d'Ivoire\n",
      "Epoch [50/200], Loss: 0.0238\n",
      "Epoch [100/200], Loss: 0.0172\n",
      "Epoch [150/200], Loss: 0.0137\n",
      "Epoch [200/200], Loss: 0.0137\n",
      "\n",
      "Training model for Croatia\n",
      "Epoch [50/200], Loss: 0.0045\n",
      "Epoch [100/200], Loss: 0.0018\n",
      "Epoch [150/200], Loss: 0.0017\n",
      "Epoch [200/200], Loss: 0.0018\n",
      "\n",
      "Training model for Cuba\n",
      "Epoch [50/200], Loss: 0.0029\n",
      "Epoch [100/200], Loss: 0.0025\n",
      "Epoch [150/200], Loss: 0.0019\n",
      "Epoch [200/200], Loss: 0.0018\n",
      "\n",
      "Training model for Curacao\n",
      "Epoch [50/200], Loss: 0.0089\n",
      "Epoch [100/200], Loss: 0.0058\n",
      "Epoch [150/200], Loss: 0.0041\n",
      "Epoch [200/200], Loss: 0.0038\n",
      "\n",
      "Training model for Cyprus\n",
      "Epoch [50/200], Loss: 0.0028\n",
      "Epoch [100/200], Loss: 0.0018\n",
      "Epoch [150/200], Loss: 0.0016\n",
      "Epoch [200/200], Loss: 0.0015\n",
      "\n",
      "Training model for Czechia\n",
      "Epoch [50/200], Loss: 0.0059\n",
      "Epoch [100/200], Loss: 0.0026\n",
      "Epoch [150/200], Loss: 0.0015\n",
      "Epoch [200/200], Loss: 0.0017\n",
      "\n",
      "Training model for Denmark\n",
      "Epoch [50/200], Loss: 0.0116\n",
      "Epoch [100/200], Loss: 0.0025\n",
      "Epoch [150/200], Loss: 0.0014\n",
      "Epoch [200/200], Loss: 0.0015\n",
      "\n",
      "Training model for Djibouti\n",
      "Epoch [50/200], Loss: 0.0108\n",
      "Epoch [100/200], Loss: 0.0045\n",
      "Epoch [150/200], Loss: 0.0033\n",
      "Epoch [200/200], Loss: 0.0037\n",
      "\n",
      "Training model for Dominica\n",
      "Epoch [50/200], Loss: 0.0056\n",
      "Epoch [100/200], Loss: 0.0048\n",
      "Epoch [150/200], Loss: 0.0047\n",
      "Epoch [200/200], Loss: 0.0036\n",
      "\n",
      "Training model for Dominican Republic\n",
      "Epoch [50/200], Loss: 0.0031\n",
      "Epoch [100/200], Loss: 0.0008\n",
      "Epoch [150/200], Loss: 0.0008\n",
      "Epoch [200/200], Loss: 0.0007\n",
      "\n",
      "Training model for Ecuador\n",
      "Epoch [50/200], Loss: 0.0060\n",
      "Epoch [100/200], Loss: 0.0020\n",
      "Epoch [150/200], Loss: 0.0019\n",
      "Epoch [200/200], Loss: 0.0016\n",
      "\n",
      "Training model for Egypt, Arab Rep.\n",
      "Epoch [50/200], Loss: 0.0036\n",
      "Epoch [100/200], Loss: 0.0030\n",
      "Epoch [150/200], Loss: 0.0031\n",
      "Epoch [200/200], Loss: 0.0029\n",
      "\n",
      "Training model for El Salvador\n",
      "Epoch [50/200], Loss: 0.0015\n",
      "Epoch [100/200], Loss: 0.0014\n",
      "Epoch [150/200], Loss: 0.0015\n",
      "Epoch [200/200], Loss: 0.0013\n",
      "\n",
      "Training model for Equatorial Guinea\n",
      "Epoch [50/200], Loss: 0.0032\n",
      "Epoch [100/200], Loss: 0.0024\n",
      "Epoch [150/200], Loss: 0.0021\n",
      "Epoch [200/200], Loss: 0.0020\n",
      "\n",
      "Training model for Eritrea\n",
      "Epoch [50/200], Loss: 0.0030\n",
      "Epoch [100/200], Loss: 0.0031\n",
      "Epoch [150/200], Loss: 0.0043\n",
      "Epoch [200/200], Loss: 0.0036\n",
      "\n",
      "Training model for Estonia\n",
      "Epoch [50/200], Loss: 0.0071\n",
      "Epoch [100/200], Loss: 0.0034\n",
      "Epoch [150/200], Loss: 0.0024\n",
      "Epoch [200/200], Loss: 0.0020\n",
      "\n",
      "Training model for Eswatini\n",
      "Epoch [50/200], Loss: 0.0028\n",
      "Epoch [100/200], Loss: 0.0033\n",
      "Epoch [150/200], Loss: 0.0027\n",
      "Epoch [200/200], Loss: 0.0031\n",
      "\n",
      "Training model for Ethiopia\n",
      "Epoch [50/200], Loss: 0.0205\n",
      "Epoch [100/200], Loss: 0.0176\n",
      "Epoch [150/200], Loss: 0.0174\n",
      "Epoch [200/200], Loss: 0.0163\n",
      "\n",
      "Training model for Faroe Islands\n",
      "Epoch [50/200], Loss: 0.0669\n",
      "Epoch [100/200], Loss: 0.0357\n",
      "Epoch [150/200], Loss: 0.0393\n",
      "Epoch [200/200], Loss: 0.0366\n",
      "\n",
      "Training model for Fiji\n",
      "Epoch [50/200], Loss: 0.0029\n",
      "Epoch [100/200], Loss: 0.0013\n",
      "Epoch [150/200], Loss: 0.0016\n",
      "Epoch [200/200], Loss: 0.0013\n",
      "\n",
      "Training model for Finland\n",
      "Epoch [50/200], Loss: 0.0061\n",
      "Epoch [100/200], Loss: 0.0014\n",
      "Epoch [150/200], Loss: 0.0012\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "\n",
      "Training model for France\n",
      "Epoch [50/200], Loss: 0.0118\n",
      "Epoch [100/200], Loss: 0.0035\n",
      "Epoch [150/200], Loss: 0.0024\n",
      "Epoch [200/200], Loss: 0.0023\n",
      "\n",
      "Training model for French Polynesia\n",
      "Epoch [50/200], Loss: 0.0031\n",
      "Epoch [100/200], Loss: 0.0017\n",
      "Epoch [150/200], Loss: 0.0016\n",
      "Epoch [200/200], Loss: 0.0013\n",
      "\n",
      "Training model for Gabon\n",
      "Epoch [50/200], Loss: 0.0046\n",
      "Epoch [100/200], Loss: 0.0011\n",
      "Epoch [150/200], Loss: 0.0010\n",
      "Epoch [200/200], Loss: 0.0009\n",
      "\n",
      "Training model for Gambia, The\n",
      "Epoch [50/200], Loss: 0.0007\n",
      "Epoch [100/200], Loss: 0.0008\n",
      "Epoch [150/200], Loss: 0.0010\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "\n",
      "Training model for Georgia\n",
      "Epoch [50/200], Loss: 0.0052\n",
      "Epoch [100/200], Loss: 0.0016\n",
      "Epoch [150/200], Loss: 0.0012\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "\n",
      "Training model for Germany\n",
      "Epoch [50/200], Loss: 0.0096\n",
      "Epoch [100/200], Loss: 0.0018\n",
      "Epoch [150/200], Loss: 0.0011\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "\n",
      "Training model for Ghana\n",
      "Epoch [50/200], Loss: 0.0056\n",
      "Epoch [100/200], Loss: 0.0042\n",
      "Epoch [150/200], Loss: 0.0039\n",
      "Epoch [200/200], Loss: 0.0038\n",
      "\n",
      "Training model for Gibraltar\n",
      "Epoch [50/200], Loss: 0.0478\n",
      "Epoch [100/200], Loss: 0.0370\n",
      "Epoch [150/200], Loss: 0.0321\n",
      "Epoch [200/200], Loss: 0.0264\n",
      "\n",
      "Training model for Greece\n",
      "Epoch [50/200], Loss: 0.0037\n",
      "Epoch [100/200], Loss: 0.0009\n",
      "Epoch [150/200], Loss: 0.0009\n",
      "Epoch [200/200], Loss: 0.0007\n",
      "\n",
      "Training model for Greenland\n",
      "Epoch [50/200], Loss: 0.0088\n",
      "Epoch [100/200], Loss: 0.0058\n",
      "Epoch [150/200], Loss: 0.0039\n",
      "Epoch [200/200], Loss: 0.0030\n",
      "\n",
      "Training model for Grenada\n",
      "Epoch [50/200], Loss: 0.0043\n",
      "Epoch [100/200], Loss: 0.0034\n",
      "Epoch [150/200], Loss: 0.0031\n",
      "Epoch [200/200], Loss: 0.0034\n",
      "\n",
      "Training model for Guam\n",
      "Epoch [50/200], Loss: 0.0073\n",
      "Epoch [100/200], Loss: 0.0054\n",
      "Epoch [150/200], Loss: 0.0054\n",
      "Epoch [200/200], Loss: 0.0049\n",
      "\n",
      "Training model for Guatemala\n",
      "Epoch [50/200], Loss: 0.0035\n",
      "Epoch [100/200], Loss: 0.0028\n",
      "Epoch [150/200], Loss: 0.0030\n",
      "Epoch [200/200], Loss: 0.0028\n",
      "\n",
      "Training model for Guinea\n",
      "Epoch [50/200], Loss: 0.0070\n",
      "Epoch [100/200], Loss: 0.0073\n",
      "Epoch [150/200], Loss: 0.0064\n",
      "Epoch [200/200], Loss: 0.0052\n",
      "\n",
      "Training model for Guinea-Bissau\n",
      "Epoch [50/200], Loss: 0.0088\n",
      "Epoch [100/200], Loss: 0.0053\n",
      "Epoch [150/200], Loss: 0.0051\n",
      "Epoch [200/200], Loss: 0.0058\n",
      "\n",
      "Training model for Guyana\n",
      "Epoch [50/200], Loss: 0.0061\n",
      "Epoch [100/200], Loss: 0.0049\n",
      "Epoch [150/200], Loss: 0.0054\n",
      "Epoch [200/200], Loss: 0.0042\n",
      "\n",
      "Training model for Haiti\n",
      "Epoch [50/200], Loss: 0.0057\n",
      "Epoch [100/200], Loss: 0.0064\n",
      "Epoch [150/200], Loss: 0.0050\n",
      "Epoch [200/200], Loss: 0.0050\n",
      "\n",
      "Training model for Honduras\n",
      "Epoch [50/200], Loss: 0.0015\n",
      "Epoch [100/200], Loss: 0.0013\n",
      "Epoch [150/200], Loss: 0.0013\n",
      "Epoch [200/200], Loss: 0.0014\n",
      "\n",
      "Training model for Hong Kong SAR, China\n",
      "Epoch [50/200], Loss: 0.0074\n",
      "Epoch [100/200], Loss: 0.0036\n",
      "Epoch [150/200], Loss: 0.0028\n",
      "Epoch [200/200], Loss: 0.0021\n",
      "\n",
      "Training model for Hungary\n",
      "Epoch [50/200], Loss: 0.0078\n",
      "Epoch [100/200], Loss: 0.0030\n",
      "Epoch [150/200], Loss: 0.0021\n",
      "Epoch [200/200], Loss: 0.0016\n",
      "\n",
      "Training model for Iceland\n",
      "Epoch [50/200], Loss: 0.0091\n",
      "Epoch [100/200], Loss: 0.0018\n",
      "Epoch [150/200], Loss: 0.0017\n",
      "Epoch [200/200], Loss: 0.0017\n",
      "\n",
      "Training model for India\n",
      "Epoch [50/200], Loss: 0.0127\n",
      "Epoch [100/200], Loss: 0.0112\n",
      "Epoch [150/200], Loss: 0.0105\n",
      "Epoch [200/200], Loss: 0.0089\n",
      "\n",
      "Training model for Indonesia\n",
      "Epoch [50/200], Loss: 0.0032\n",
      "Epoch [100/200], Loss: 0.0008\n",
      "Epoch [150/200], Loss: 0.0006\n",
      "Epoch [200/200], Loss: 0.0006\n",
      "\n",
      "Training model for Iran, Islamic Rep.\n",
      "Epoch [50/200], Loss: 0.0086\n",
      "Epoch [100/200], Loss: 0.0015\n",
      "Epoch [150/200], Loss: 0.0012\n",
      "Epoch [200/200], Loss: 0.0009\n",
      "\n",
      "Training model for Iraq\n",
      "Epoch [50/200], Loss: 0.0034\n",
      "Epoch [100/200], Loss: 0.0031\n",
      "Epoch [150/200], Loss: 0.0037\n",
      "Epoch [200/200], Loss: 0.0027\n",
      "\n",
      "Training model for Ireland\n",
      "Epoch [50/200], Loss: 0.0085\n",
      "Epoch [100/200], Loss: 0.0028\n",
      "Epoch [150/200], Loss: 0.0028\n",
      "Epoch [200/200], Loss: 0.0027\n",
      "\n",
      "Training model for Isle of Man\n",
      "Epoch [50/200], Loss: 0.0068\n",
      "Epoch [100/200], Loss: 0.0028\n",
      "Epoch [150/200], Loss: 0.0029\n",
      "Epoch [200/200], Loss: 0.0032\n",
      "\n",
      "Training model for Israel\n",
      "Epoch [50/200], Loss: 0.0118\n",
      "Epoch [100/200], Loss: 0.0063\n",
      "Epoch [150/200], Loss: 0.0068\n",
      "Epoch [200/200], Loss: 0.0059\n",
      "\n",
      "Training model for Italy\n",
      "Epoch [50/200], Loss: 0.0047\n",
      "Epoch [100/200], Loss: 0.0046\n",
      "Epoch [150/200], Loss: 0.0038\n",
      "Epoch [200/200], Loss: 0.0039\n",
      "\n",
      "Training model for Jamaica\n",
      "Epoch [50/200], Loss: 0.0033\n",
      "Epoch [100/200], Loss: 0.0026\n",
      "Epoch [150/200], Loss: 0.0026\n",
      "Epoch [200/200], Loss: 0.0024\n",
      "\n",
      "Training model for Japan\n",
      "Epoch [50/200], Loss: 0.0093\n",
      "Epoch [100/200], Loss: 0.0024\n",
      "Epoch [150/200], Loss: 0.0019\n",
      "Epoch [200/200], Loss: 0.0017\n",
      "\n",
      "Training model for Jordan\n",
      "Epoch [50/200], Loss: 0.0011\n",
      "Epoch [100/200], Loss: 0.0007\n",
      "Epoch [150/200], Loss: 0.0006\n",
      "Epoch [200/200], Loss: 0.0006\n",
      "\n",
      "Training model for Kazakhstan\n",
      "Epoch [50/200], Loss: 0.0188\n",
      "Epoch [100/200], Loss: 0.0042\n",
      "Epoch [150/200], Loss: 0.0024\n",
      "Epoch [200/200], Loss: 0.0012\n",
      "\n",
      "Training model for Kenya\n",
      "Epoch [50/200], Loss: 0.0061\n",
      "Epoch [100/200], Loss: 0.0033\n",
      "Epoch [150/200], Loss: 0.0024\n",
      "Epoch [200/200], Loss: 0.0020\n",
      "\n",
      "Training model for Kiribati\n",
      "Epoch [50/200], Loss: 0.0031\n",
      "Epoch [100/200], Loss: 0.0031\n",
      "Epoch [150/200], Loss: 0.0026\n",
      "Epoch [200/200], Loss: 0.0037\n",
      "\n",
      "Training model for Korea, Dem. People's Rep.\n",
      "Epoch [50/200], Loss: 0.0041\n",
      "Epoch [100/200], Loss: 0.0044\n",
      "Epoch [150/200], Loss: 0.0044\n",
      "Epoch [200/200], Loss: 0.0033\n",
      "\n",
      "Training model for Korea, Rep.\n",
      "Epoch [50/200], Loss: 0.0139\n",
      "Epoch [100/200], Loss: 0.0091\n",
      "Epoch [150/200], Loss: 0.0074\n",
      "Epoch [200/200], Loss: 0.0040\n",
      "\n",
      "Training model for Kosovo\n",
      "Epoch [50/200], Loss: 0.0120\n",
      "Epoch [100/200], Loss: 0.0079\n",
      "Epoch [150/200], Loss: 0.0051\n",
      "Epoch [200/200], Loss: 0.0033\n",
      "\n",
      "Training model for Kuwait\n",
      "Epoch [50/200], Loss: 0.0037\n",
      "Epoch [100/200], Loss: 0.0012\n",
      "Epoch [150/200], Loss: 0.0011\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "\n",
      "Training model for Kyrgyz Republic\n",
      "Epoch [50/200], Loss: 0.0039\n",
      "Epoch [100/200], Loss: 0.0022\n",
      "Epoch [150/200], Loss: 0.0021\n",
      "Epoch [200/200], Loss: 0.0022\n",
      "\n",
      "Training model for Lao PDR\n",
      "Epoch [50/200], Loss: 0.0022\n",
      "Epoch [100/200], Loss: 0.0013\n",
      "Epoch [150/200], Loss: 0.0012\n",
      "Epoch [200/200], Loss: 0.0012\n",
      "\n",
      "Training model for Latvia\n",
      "Epoch [50/200], Loss: 0.0096\n",
      "Epoch [100/200], Loss: 0.0031\n",
      "Epoch [150/200], Loss: 0.0020\n",
      "Epoch [200/200], Loss: 0.0016\n",
      "\n",
      "Training model for Lebanon\n",
      "Epoch [50/200], Loss: 0.0067\n",
      "Epoch [100/200], Loss: 0.0022\n",
      "Epoch [150/200], Loss: 0.0018\n",
      "Epoch [200/200], Loss: 0.0018\n",
      "\n",
      "Training model for Lesotho\n",
      "Epoch [50/200], Loss: 0.0077\n",
      "Epoch [100/200], Loss: 0.0070\n",
      "Epoch [150/200], Loss: 0.0075\n",
      "Epoch [200/200], Loss: 0.0069\n",
      "\n",
      "Training model for Liberia\n",
      "Epoch [50/200], Loss: 0.0078\n",
      "Epoch [100/200], Loss: 0.0072\n",
      "Epoch [150/200], Loss: 0.0068\n",
      "Epoch [200/200], Loss: 0.0067\n",
      "\n",
      "Training model for Libya\n",
      "Epoch [50/200], Loss: 0.0302\n",
      "Epoch [100/200], Loss: 0.0257\n",
      "Epoch [150/200], Loss: 0.0160\n",
      "Epoch [200/200], Loss: 0.0154\n",
      "\n",
      "Training model for Liechtenstein\n",
      "Epoch [50/200], Loss: 0.0382\n",
      "Epoch [100/200], Loss: 0.0438\n",
      "Epoch [150/200], Loss: 0.0300\n",
      "Epoch [200/200], Loss: 0.0287\n",
      "\n",
      "Training model for Lithuania\n",
      "Epoch [50/200], Loss: 0.0069\n",
      "Epoch [100/200], Loss: 0.0028\n",
      "Epoch [150/200], Loss: 0.0021\n",
      "Epoch [200/200], Loss: 0.0017\n",
      "\n",
      "Training model for Luxembourg\n",
      "Epoch [50/200], Loss: 0.0073\n",
      "Epoch [100/200], Loss: 0.0008\n",
      "Epoch [150/200], Loss: 0.0004\n",
      "Epoch [200/200], Loss: 0.0004\n",
      "\n",
      "Training model for Macao SAR, China\n",
      "Epoch [50/200], Loss: 0.0033\n",
      "Epoch [100/200], Loss: 0.0021\n",
      "Epoch [150/200], Loss: 0.0017\n",
      "Epoch [200/200], Loss: 0.0016\n",
      "\n",
      "Training model for Madagascar\n",
      "Epoch [50/200], Loss: 0.0151\n",
      "Epoch [100/200], Loss: 0.0056\n",
      "Epoch [150/200], Loss: 0.0035\n",
      "Epoch [200/200], Loss: 0.0018\n",
      "\n",
      "Training model for Malawi\n",
      "Epoch [50/200], Loss: 0.0165\n",
      "Epoch [100/200], Loss: 0.0117\n",
      "Epoch [150/200], Loss: 0.0116\n",
      "Epoch [200/200], Loss: 0.0080\n",
      "\n",
      "Training model for Malaysia\n",
      "Epoch [50/200], Loss: 0.0056\n",
      "Epoch [100/200], Loss: 0.0050\n",
      "Epoch [150/200], Loss: 0.0046\n",
      "Epoch [200/200], Loss: 0.0042\n",
      "\n",
      "Training model for Maldives\n",
      "Epoch [50/200], Loss: 0.0034\n",
      "Epoch [100/200], Loss: 0.0009\n",
      "Epoch [150/200], Loss: 0.0009\n",
      "Epoch [200/200], Loss: 0.0009\n",
      "\n",
      "Training model for Mali\n",
      "Epoch [50/200], Loss: 0.0084\n",
      "Epoch [100/200], Loss: 0.0074\n",
      "Epoch [150/200], Loss: 0.0073\n",
      "Epoch [200/200], Loss: 0.0080\n",
      "\n",
      "Training model for Malta\n",
      "Epoch [50/200], Loss: 0.0038\n",
      "Epoch [100/200], Loss: 0.0017\n",
      "Epoch [150/200], Loss: 0.0016\n",
      "Epoch [200/200], Loss: 0.0014\n",
      "\n",
      "Training model for Marshall Islands\n",
      "Epoch [50/200], Loss: 0.0171\n",
      "Epoch [100/200], Loss: 0.0103\n",
      "Epoch [150/200], Loss: 0.0070\n",
      "Epoch [200/200], Loss: 0.0041\n",
      "\n",
      "Training model for Mauritania\n",
      "Epoch [50/200], Loss: 0.0058\n",
      "Epoch [100/200], Loss: 0.0051\n",
      "Epoch [150/200], Loss: 0.0065\n",
      "Epoch [200/200], Loss: 0.0048\n",
      "\n",
      "Training model for Mauritius\n",
      "Epoch [50/200], Loss: 0.0023\n",
      "Epoch [100/200], Loss: 0.0014\n",
      "Epoch [150/200], Loss: 0.0014\n",
      "Epoch [200/200], Loss: 0.0015\n",
      "\n",
      "Training model for Mexico\n",
      "Epoch [50/200], Loss: 0.0031\n",
      "Epoch [100/200], Loss: 0.0019\n",
      "Epoch [150/200], Loss: 0.0017\n",
      "Epoch [200/200], Loss: 0.0016\n",
      "\n",
      "Training model for Micronesia, Fed. Sts.\n",
      "Epoch [50/200], Loss: 0.0063\n",
      "Epoch [100/200], Loss: 0.0068\n",
      "Epoch [150/200], Loss: 0.0091\n",
      "Epoch [200/200], Loss: 0.0059\n",
      "\n",
      "Training model for Moldova\n",
      "Epoch [50/200], Loss: 0.0146\n",
      "Epoch [100/200], Loss: 0.0108\n",
      "Epoch [150/200], Loss: 0.0092\n",
      "Epoch [200/200], Loss: 0.0091\n",
      "\n",
      "Training model for Monaco\n",
      "Epoch [50/200], Loss: 0.0224\n",
      "Epoch [100/200], Loss: 0.0234\n",
      "Epoch [150/200], Loss: 0.0233\n",
      "Epoch [200/200], Loss: 0.0188\n",
      "\n",
      "Training model for Mongolia\n",
      "Epoch [50/200], Loss: 0.0484\n",
      "Epoch [100/200], Loss: 0.0343\n",
      "Epoch [150/200], Loss: 0.0309\n",
      "Epoch [200/200], Loss: 0.0239\n",
      "\n",
      "Training model for Montenegro\n",
      "Epoch [50/200], Loss: 0.0127\n",
      "Epoch [100/200], Loss: 0.0130\n",
      "Epoch [150/200], Loss: 0.0122\n",
      "Epoch [200/200], Loss: 0.0119\n",
      "\n",
      "Training model for Morocco\n",
      "Epoch [50/200], Loss: 0.0099\n",
      "Epoch [100/200], Loss: 0.0060\n",
      "Epoch [150/200], Loss: 0.0058\n",
      "Epoch [200/200], Loss: 0.0047\n",
      "\n",
      "Training model for Mozambique\n",
      "Epoch [50/200], Loss: 0.0128\n",
      "Epoch [100/200], Loss: 0.0070\n",
      "Epoch [150/200], Loss: 0.0053\n",
      "Epoch [200/200], Loss: 0.0032\n",
      "\n",
      "Training model for Myanmar\n",
      "Epoch [50/200], Loss: 0.0152\n",
      "Epoch [100/200], Loss: 0.0101\n",
      "Epoch [150/200], Loss: 0.0119\n",
      "Epoch [200/200], Loss: 0.0086\n",
      "\n",
      "Training model for Namibia\n",
      "Epoch [50/200], Loss: 0.0036\n",
      "Epoch [100/200], Loss: 0.0013\n",
      "Epoch [150/200], Loss: 0.0013\n",
      "Epoch [200/200], Loss: 0.0013\n",
      "\n",
      "Training model for Nauru\n",
      "Epoch [50/200], Loss: 0.0268\n",
      "Epoch [100/200], Loss: 0.0212\n",
      "Epoch [150/200], Loss: 0.0218\n",
      "Epoch [200/200], Loss: 0.0198\n",
      "\n",
      "Training model for Nepal\n",
      "Epoch [50/200], Loss: 0.0021\n",
      "Epoch [100/200], Loss: 0.0027\n",
      "Epoch [150/200], Loss: 0.0023\n",
      "Epoch [200/200], Loss: 0.0026\n",
      "\n",
      "Training model for Netherlands\n",
      "Epoch [50/200], Loss: 0.0072\n",
      "Epoch [100/200], Loss: 0.0017\n",
      "Epoch [150/200], Loss: 0.0013\n",
      "Epoch [200/200], Loss: 0.0010\n",
      "\n",
      "Training model for New Caledonia\n",
      "Epoch [50/200], Loss: 0.0066\n",
      "Epoch [100/200], Loss: 0.0043\n",
      "Epoch [150/200], Loss: 0.0040\n",
      "Epoch [200/200], Loss: 0.0038\n",
      "\n",
      "Training model for New Zealand\n",
      "Epoch [50/200], Loss: 0.0060\n",
      "Epoch [100/200], Loss: 0.0046\n",
      "Epoch [150/200], Loss: 0.0037\n",
      "Epoch [200/200], Loss: 0.0028\n",
      "\n",
      "Training model for Nicaragua\n",
      "Epoch [50/200], Loss: 0.0021\n",
      "Epoch [100/200], Loss: 0.0022\n",
      "Epoch [150/200], Loss: 0.0014\n",
      "Epoch [200/200], Loss: 0.0015\n",
      "\n",
      "Training model for Niger\n",
      "Epoch [50/200], Loss: 0.0140\n",
      "Epoch [100/200], Loss: 0.0101\n",
      "Epoch [150/200], Loss: 0.0060\n",
      "Epoch [200/200], Loss: 0.0056\n",
      "\n",
      "Training model for Nigeria\n",
      "Epoch [50/200], Loss: 0.0098\n",
      "Epoch [100/200], Loss: 0.0099\n",
      "Epoch [150/200], Loss: 0.0135\n",
      "Epoch [200/200], Loss: 0.0091\n",
      "\n",
      "Training model for North Macedonia\n",
      "Epoch [50/200], Loss: 0.0055\n",
      "Epoch [100/200], Loss: 0.0024\n",
      "Epoch [150/200], Loss: 0.0024\n",
      "Epoch [200/200], Loss: 0.0023\n",
      "\n",
      "Training model for Northern Mariana Islands\n",
      "Epoch [50/200], Loss: 0.0056\n",
      "Epoch [100/200], Loss: 0.0029\n",
      "Epoch [150/200], Loss: 0.0027\n",
      "Epoch [200/200], Loss: 0.0024\n",
      "\n",
      "Training model for Norway\n",
      "Epoch [50/200], Loss: 0.0079\n",
      "Epoch [100/200], Loss: 0.0022\n",
      "Epoch [150/200], Loss: 0.0011\n",
      "Epoch [200/200], Loss: 0.0010\n",
      "\n",
      "Training model for Oman\n",
      "Epoch [50/200], Loss: 0.0054\n",
      "Epoch [100/200], Loss: 0.0018\n",
      "Epoch [150/200], Loss: 0.0022\n",
      "Epoch [200/200], Loss: 0.0017\n",
      "\n",
      "Training model for Pakistan\n",
      "Epoch [50/200], Loss: 0.0193\n",
      "Epoch [100/200], Loss: 0.0068\n",
      "Epoch [150/200], Loss: 0.0033\n",
      "Epoch [200/200], Loss: 0.0021\n",
      "\n",
      "Training model for Palau\n",
      "Epoch [50/200], Loss: 0.0252\n",
      "Epoch [100/200], Loss: 0.0210\n",
      "Epoch [150/200], Loss: 0.0151\n",
      "Epoch [200/200], Loss: 0.0109\n",
      "\n",
      "Training model for Panama\n",
      "Epoch [50/200], Loss: 0.0043\n",
      "Epoch [100/200], Loss: 0.0040\n",
      "Epoch [150/200], Loss: 0.0040\n",
      "Epoch [200/200], Loss: 0.0035\n",
      "\n",
      "Training model for Papua New Guinea\n",
      "Epoch [50/200], Loss: 0.0219\n",
      "Epoch [100/200], Loss: 0.0200\n",
      "Epoch [150/200], Loss: 0.0133\n",
      "Epoch [200/200], Loss: 0.0101\n",
      "\n",
      "Training model for Paraguay\n",
      "Epoch [50/200], Loss: 0.0041\n",
      "Epoch [100/200], Loss: 0.0006\n",
      "Epoch [150/200], Loss: 0.0004\n",
      "Epoch [200/200], Loss: 0.0004\n",
      "\n",
      "Training model for Peru\n",
      "Epoch [50/200], Loss: 0.0028\n",
      "Epoch [100/200], Loss: 0.0025\n",
      "Epoch [150/200], Loss: 0.0020\n",
      "Epoch [200/200], Loss: 0.0018\n",
      "\n",
      "Training model for Philippines\n",
      "Epoch [50/200], Loss: 0.0074\n",
      "Epoch [100/200], Loss: 0.0078\n",
      "Epoch [150/200], Loss: 0.0056\n",
      "Epoch [200/200], Loss: 0.0060\n",
      "\n",
      "Training model for Poland\n",
      "Epoch [50/200], Loss: 0.0055\n",
      "Epoch [100/200], Loss: 0.0025\n",
      "Epoch [150/200], Loss: 0.0018\n",
      "Epoch [200/200], Loss: 0.0013\n",
      "\n",
      "Training model for Portugal\n",
      "Epoch [50/200], Loss: 0.0025\n",
      "Epoch [100/200], Loss: 0.0008\n",
      "Epoch [150/200], Loss: 0.0008\n",
      "Epoch [200/200], Loss: 0.0007\n",
      "\n",
      "Training model for Puerto Rico\n",
      "Epoch [50/200], Loss: 0.0067\n",
      "Epoch [100/200], Loss: 0.0046\n",
      "Epoch [150/200], Loss: 0.0042\n",
      "Epoch [200/200], Loss: 0.0048\n",
      "\n",
      "Training model for Qatar\n",
      "Epoch [50/200], Loss: 0.0076\n",
      "Epoch [100/200], Loss: 0.0015\n",
      "Epoch [150/200], Loss: 0.0015\n",
      "Epoch [200/200], Loss: 0.0016\n",
      "\n",
      "Training model for Romania\n",
      "Epoch [50/200], Loss: 0.0034\n",
      "Epoch [100/200], Loss: 0.0021\n",
      "Epoch [150/200], Loss: 0.0017\n",
      "Epoch [200/200], Loss: 0.0015\n",
      "\n",
      "Training model for Russian Federation\n",
      "Epoch [50/200], Loss: 0.0083\n",
      "Epoch [100/200], Loss: 0.0012\n",
      "Epoch [150/200], Loss: 0.0015\n",
      "Epoch [200/200], Loss: 0.0011\n",
      "\n",
      "Training model for Rwanda\n",
      "Epoch [50/200], Loss: 0.0089\n",
      "Epoch [100/200], Loss: 0.0057\n",
      "Epoch [150/200], Loss: 0.0047\n",
      "Epoch [200/200], Loss: 0.0041\n",
      "\n",
      "Training model for Samoa\n",
      "Epoch [50/200], Loss: 0.0009\n",
      "Epoch [100/200], Loss: 0.0007\n",
      "Epoch [150/200], Loss: 0.0009\n",
      "Epoch [200/200], Loss: 0.0006\n",
      "\n",
      "Training model for San Marino\n",
      "Epoch [50/200], Loss: 0.0420\n",
      "Epoch [100/200], Loss: 0.0245\n",
      "Epoch [150/200], Loss: 0.0231\n",
      "Epoch [200/200], Loss: 0.0236\n",
      "\n",
      "Training model for Sao Tome and Principe\n",
      "Epoch [50/200], Loss: 0.0019\n",
      "Epoch [100/200], Loss: 0.0020\n",
      "Epoch [150/200], Loss: 0.0017\n",
      "Epoch [200/200], Loss: 0.0016\n",
      "\n",
      "Training model for Saudi Arabia\n",
      "Epoch [50/200], Loss: 0.0024\n",
      "Epoch [100/200], Loss: 0.0021\n",
      "Epoch [150/200], Loss: 0.0021\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m X, y, scaler \u001b[38;5;241m=\u001b[39m prepare_country_data(country_data, sequence_length)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Check if we have enough data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     last_sequence \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(country_data[\u001b[38;5;241m-\u001b[39msequence_length:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X, y, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/optim/adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for country in df['Country']:\n",
    "        print(f\"\\nTraining model for {country}\")\n",
    "        \n",
    "        # Get country data\n",
    "        country_data = df[df['Country'] == country][cols].values.flatten()\n",
    "\n",
    "        country_data = country_data.astype(float)\n",
    "        \n",
    "        # Prepare data\n",
    "        X, y, scaler = prepare_country_data(country_data, sequence_length)\n",
    "        \n",
    "        if len(X) > 0:  # Check if we have enough data\n",
    "            # Train model\n",
    "            model = train_model(X, y)\n",
    "            \n",
    "            # Make predictions\n",
    "            last_sequence = scaler.transform(country_data[-sequence_length:].reshape(-1, 1))\n",
    "            predictions = predict_future(model, last_sequence, scaler)\n",
    "            predictions_by_country[country] = predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m selected_countries:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m predictions_by_country:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for country in selected_countries:\n",
    "    if country in predictions_by_country:\n",
    "        plt.plot(range(2024, 2029), predictions_by_country[country], label=country)\n",
    "plt.title('Internet Predictions (2024-2028)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Internet (Percentage)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "    \n",
    "    # Print predictions for selected countries\n",
    "print(\"\\nPredicted Internet (2024-2028):\")\n",
    "for country in selected_countries:\n",
    "    if country in predictions_by_country:\n",
    "        print(f\"\\n{country}:\")\n",
    "        for year, pred in zip(range(2024, 2029), predictions_by_country[country]):\n",
    "            print(f\"{year}: ${pred:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions exported to lstm_datasets/internet_prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# Export predictions to CSV\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['Country'] = list(predictions_by_country.keys())\n",
    "\n",
    "for year in range(2024, 2029):\n",
    "    year_predictions = []\n",
    "    for country in predictions_df['Country']:\n",
    "        if country in predictions_by_country:\n",
    "            year_predictions.append(predictions_by_country[country][year-2024])\n",
    "        else:\n",
    "            year_predictions.append(None)\n",
    "    predictions_df[f'{year} Internet'] = year_predictions\n",
    "\n",
    "predictions_df.to_csv('../lstm_datasets/internet_prediction.csv', index=False)\n",
    "print(\"\\nPredictions exported to lstm_datasets/internet_prediction.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
